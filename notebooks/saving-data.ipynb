{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb584f96",
   "metadata": {},
   "source": [
    "# Saving Data\n",
    "\n",
    "## First, load some data ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98f16f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "spark = SparkSession.builder.master(\"local\").getOrCreate()\n",
    "DATA_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99abad99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------+----------+\n",
      "|id |firstname|lastname|dob       |\n",
      "+---+---------+--------+----------+\n",
      "|1  |John     |Smith   |2001-01-01|\n",
      "|2  |Kim      |Melly   |1998-08-28|\n",
      "+---+---------+--------+----------+\n",
      "\n",
      "+----------+---------+-----------------------+-------------+----+-----------+\n",
      "|date      |m_title  |m_body                 |m_attachments|user|recipient  |\n",
      "+----------+---------+-----------------------+-------------+----+-----------+\n",
      "|2022-01-03|Title    |Hello World            |null         |2   |p@gmail.com|\n",
      "|2022-01-02|Title 2  |Hello World            |null         |3   |d@gmail.com|\n",
      "|2022-01-03|Spark SQL|Let's learn about spark|null         |2   |a@gmail.com|\n",
      "+----------+---------+-----------------------+-------------+----+-----------+\n",
      "\n",
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users = spark.read.option(\"header\", \"true\").csv(f\"{DATA_DIR}/users.csv\")\n",
    "messages = spark.read.option(\"header\", \"true\").csv(f\"{DATA_DIR}/messages.csv\")\n",
    "users.show(10, False)\n",
    "messages.show(10, False)\n",
    "users.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a744b08",
   "metadata": {},
   "source": [
    "## Write to parquet ...\n",
    "\n",
    "We will take the CSV data read into dataframes above. Then we'll add some typing to the data (date string from CSV will become a DateType) just to show it is possible to have different types and that parquet will honour those types (whereas CSV won't). You can read more about types [here](https://sparkbyexamples.com/pyspark/pyspark-sql-types-datatype-with-examples/). We can also look at the output files created (**note** these files are renamed in source control to a consistent filename to make diffs easier to track)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d323871",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_path = f\"{DATA_DIR}/parquet/users\"\n",
    "messages_path = f\"{DATA_DIR}/parquet/messages\"\n",
    "\n",
    "(\n",
    "    users\n",
    "    .select(\"id\", \"firstname\", \"lastname\", col(\"dob\").cast(\"date\"))\n",
    "    .write.parquet(users_path, mode=\"overwrite\")\n",
    ")\n",
    "\n",
    "(\n",
    "    messages\n",
    "    .withColumn(\"dt\", col(\"date\").cast(\"date\"))\n",
    "    .drop(\"date\")\n",
    "    .write.parquet(messages_path, mode=\"overwrite\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc7fea35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- dob: date (nullable = true)\n",
      "\n",
      "root\n",
      " |-- m_title: string (nullable = true)\n",
      " |-- m_body: string (nullable = true)\n",
      " |-- m_attachments: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- recipient: string (nullable = true)\n",
      " |-- dt: date (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['._SUCCESS.crc',\n",
       " '.part-00000-d65f96af-1a05-4e78-81d3-716070df6d2b-c000.snappy.parquet.crc',\n",
       " '_SUCCESS',\n",
       " 'part-00000-d65f96af-1a05-4e78-81d3-716070df6d2b-c000.snappy.parquet']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(users_path).printSchema()\n",
    "spark.read.parquet(messages_path).printSchema()\n",
    "os.listdir(users_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcbeefa",
   "metadata": {},
   "source": [
    "## Writing parquet into partitions\n",
    "\n",
    "Here we can take all our data and write it to partitions. Often when I am working with data it is partitioned by date - so there are folders organised by date. Let's see how that looks in a temporary folder (that won't be in version control)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67f41186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['._SUCCESS.crc', '_SUCCESS', 'dt=2022-01-02', 'dt=2022-01-03']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition_path = f\"{DATA_DIR}/tmp/messages\"\n",
    "messages = spark.read.parquet(messages_path)\n",
    "messages.write.partitionBy(\"dt\").mode(\"overwrite\").parquet(partition_path)\n",
    "os.listdir(partition_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f32f157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------------+----+-----------+\n",
      "|m_title|     m_body|m_attachments|user|  recipient|\n",
      "+-------+-----------+-------------+----+-----------+\n",
      "|Title 2|Hello World|         null|   3|d@gmail.com|\n",
      "+-------+-----------+-------------+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# then to read a single partition\n",
    "spark.read.parquet(f\"{DATA_DIR}/tmp/messages/dt=2022-01-02\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f433394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from multiple partitions\n",
    "count = spark.read.parquet(f\"{DATA_DIR}/tmp/messages\").count()\n",
    "assert count == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2a6863",
   "metadata": {},
   "source": [
    "## Watch out for overwrite\n",
    "\n",
    "If you are writing to partitions (here by `dt`), we need to be careful not to overwrite all partitions.\n",
    "Currently we have data in 2 dt partitions. If I were to load a single date's data, and write it like this:\n",
    "\n",
    "```\n",
    "messages.write.partitionBy(\"dt\").mode(\"overwrite\").parquet(write_path)\n",
    "```\n",
    "\n",
    "What would happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7135437c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------------+----+-----------+----------+\n",
      "|m_title|     m_body|m_attachments|user|  recipient|        dt|\n",
      "+-------+-----------+-------------+----+-----------+----------+\n",
      "|Title 2|Hello World|         null|   3|d@gmail.com|2022-01-02|\n",
      "+-------+-----------+-------------+----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "single_dt = messages.filter(col(\"dt\") == \"2022-01-02\")\n",
    "single_dt.write.partitionBy(\"dt\").mode(\"overwrite\").parquet(f\"{DATA_DIR}/tmp/messages\")\n",
    "df = spark.read.parquet(f\"{DATA_DIR}/tmp/messages\")\n",
    "df.show() # we've lost the other partition!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283ce080",
   "metadata": {},
   "source": [
    "We can imagine a situation where:\n",
    "* we have decided to partition by date/dt\n",
    "* we want to be able to rewrite data for a particular data\n",
    "\n",
    "To achieve this, we will\n",
    "\n",
    "1. reload the original data\n",
    "2. write it again so we have 2 dt partitions\n",
    "3. adapt the data for a given dt partition and then rewrite it to the specific partition\n",
    "4. confirm that we have not lost the other partition's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36e6e504",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+-------------+----+-----------+----------+\n",
      "|  m_title|              m_body|m_attachments|user|  recipient|        dt|\n",
      "+---------+--------------------+-------------+----+-----------+----------+\n",
      "|    Title|         Hello World|         null|   2|p@gmail.com|2022-01-03|\n",
      "|Spark SQL|Let's learn about...|         null|   2|a@gmail.com|2022-01-03|\n",
      "|  Title 2|         Hello World|         null|   3|d@gmail.com|2022-01-02|\n",
      "+---------+--------------------+-------------+----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# step 1\n",
    "data = spark.read.parquet(messages_path)\n",
    "assert data.count() == 3\n",
    "data.write.partitionBy(\"dt\").mode(\"overwrite\").parquet(f\"{DATA_DIR}/tmp/messages\") # step 2\n",
    "partitioned_data = spark.read.parquet(f\"{DATA_DIR}/tmp/messages\")\n",
    "partitioned_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47b24b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----+-----------+----------+-------------+\n",
      "|     m_body|m_attachments|user|  recipient|        dt|      m_title|\n",
      "+-----------+-------------+----+-----------+----------+-------------+\n",
      "|Hello World|         null|   3|d@gmail.com|2022-01-02|Updated title|\n",
      "+-----------+-------------+----+-----------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# step 3\n",
    "new_row = (\n",
    "    messages.filter(col(\"dt\") == \"2022-01-02\")\n",
    "    .drop(\"m_title\")\n",
    "    .withColumn(\"m_title\", lit(\"Updated title\"))\n",
    ")\n",
    "new_row.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6f882c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    new_row\n",
    "    .write.mode(\"overwrite\")\n",
    "    .parquet(f\"{DATA_DIR}/tmp/messages/dt=2022-01-02\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e68e1d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+----+-----------+----------+-------------+\n",
      "|              m_body|m_attachments|user|  recipient|        dt|      m_title|\n",
      "+--------------------+-------------+----+-----------+----------+-------------+\n",
      "|         Hello World|         null|   3|d@gmail.com|2022-01-02|Updated title|\n",
      "|         Hello World|         null|   2|p@gmail.com|2022-01-03|        Title|\n",
      "|Let's learn about...|         null|   2|a@gmail.com|2022-01-03|    Spark SQL|\n",
      "+--------------------+-------------+----+-----------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The record for 2022-01-02 should have n_title of \"Updated title\"\n",
    "spark.read.parquet(f\"{DATA_DIR}/tmp/messages\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d911409",
   "metadata": {},
   "source": [
    "## Writing to other formats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fca4dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as ... \n",
    "# json\n",
    "json_path = f\"{DATA_DIR}/tmp/messages-json\"\n",
    "messages.write.format(\"json\").mode(\"overwrite\").save(json_path)\n",
    "# csv\n",
    "csv_path = f\"{DATA_DIR}/tmp/messages-csv\"\n",
    "messages.write.format(\"csv\").mode(\"overwrite\").save(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718b1cff",
   "metadata": {},
   "source": [
    "which look like (note how the null attachments were not written)\n",
    "\n",
    "```\n",
    "{\"m_title\":\"Title\",\"m_body\":\"Hello World\",\"user\":\"2\",\"recipient\":\"p@gmail.com\",\"dt\":\"2022-01-03\"}\n",
    "{\"m_title\":\"Title 2\",\"m_body\":\"Hello World\",\"user\":\"3\",\"recipient\":\"d@gmail.com\",\"dt\":\"2022-01-02\"}\n",
    "{\"m_title\":\"Spark SQL\",\"m_body\":\"Let's learn about spark\",\"user\":\"2\",\"recipient\":\"a@gmail.com\",\"dt\":\"2022-01-03\"}\n",
    "```\n",
    "\n",
    "and\n",
    "\n",
    "```\n",
    "Title,Hello World,\"\",2,p@gmail.com,2022-01-03\n",
    "Title 2,Hello World,\"\",3,d@gmail.com,2022-01-02\n",
    "Spark SQL,Let's learn about spark,\"\",2,a@gmail.com,2022-01-03\n",
    "\n",
    "```\n",
    "\n",
    "If we want to force nulls to be written to JSON, we need to set an option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f34b820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.write.format(\"json\").mode(\"overwrite\").option(\"ignoreNullFields\", \"false\").save(json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e8d3b7",
   "metadata": {},
   "source": [
    "```\n",
    "{\"m_title\":\"Title\",\"m_body\":\"Hello World\",\"m_attachments\":null,\"user\":\"2\",\"recipient\":\"p@gmail.com\",\"dt\":\"2022-01-03\"}\n",
    "{\"m_title\":\"Title 2\",\"m_body\":\"Hello World\",\"m_attachments\":null,\"user\":\"3\",\"recipient\":\"d@gmail.com\",\"dt\":\"2022-01-02\"}\n",
    "{\"m_title\":\"Spark SQL\",\"m_body\":\"Let's learn about spark\",\"m_attachments\":null,\"user\":\"2\",\"recipient\":\"a@gmail.com\",\"dt\":\"2022-01-03\"}\n",
    "\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
